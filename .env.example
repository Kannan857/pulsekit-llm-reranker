# ==== LLM (choose one) ====
# FP model (more VRAM): LLM_MODEL="meta-llama/Llama-3.1-8B-Instruct" ; LLM_QUANTIZATION=""
LLM_MODEL="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
LLM_QUANTIZATION="awq"

VLLM_TP="1"
VLLM_MAX_LEN="8192"
VLLM_GPU_UTIL="0.90"
VLLM_TIMEOUT="90"

# HF authentication for Meta models and some mirrors
HUGGINGFACE_HUB_TOKEN="<set-in-deploy-env>"

# ==== Reranker (tiny, CPU by default) ====
RERANKER_MODEL="cross-encoder/ms-marco-MiniLM-L-6-v2"
RERANKER_DEVICE="cpu"

# Optional caches (mounted as volume in RunPod)
HF_HOME="/root/.cache/huggingface"
TRANSFORMERS_CACHE="/root/.cache/huggingface"
